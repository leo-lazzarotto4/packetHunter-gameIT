import os
import sys
import argparse
import json
import csv
import time
import re
import ipaddress
import hashlib
import base64
import statistics
import random
import datetime
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Set, Any, Optional, Union
import concurrent.futures
from functools import lru_cache

# Bibliothèques de traitement de paquets réseau
import pyshark
import scapy.all as scapy
from scapy.layers import http, dns, tls
import dpkt
from dpkt.utils import inet_to_str, mac_to_str
import dns.resolver
import dns.name

# Analyse de certificats
from cryptography import x509
from cryptography.hazmat.backends import default_backend
from cryptography.x509.oid import NameOID, ExtensionOID

# Analyse de données
import pandas as pd
import numpy as np

# Intégration avec APIs externes
import requests
from requests.exceptions import RequestException

# Barre de progression
from tqdm import tqdm

# Désactivation de tous les logs
import logging
logging.getLogger().setLevel(logging.CRITICAL)
for name in logging.root.manager.loggerDict:
    logging.getLogger(name).setLevel(logging.CRITICAL)
    logging.getLogger(name).propagate = False

# Constantes globales (inchangées)
DEFAULT_SSL_PORTS = {443, 8443, 465, 993, 995, 5061, 5223}
SUSPICIOUS_UA_PATTERNS = [
    r"(?i)(wget|curl|python-requests|go-http|perl|ruby)",
    r"(?i)(Mozilla/[0-9]\.[0-9] \(compatible;)",
    r"(?i)(Mozilla/5\.0 \(Windows NT [0-9]\.[0-9]; Trident/7\.0;)"
]
SUSPICIOUS_DOMAINS_ENTROPY_THRESHOLD = 0.8
DNS_TUNNELING_ENTROPY_THRESHOLD = 0.75
DNS_SUBDOMAIN_DEPTH_THRESHOLD = 5
HTTP_SUSPICIOUS_PATHS = [
    r"(?i)/admin/",
    r"(?i)/shell",
    r"(?i)/cmd",
    r"(?i)\.php$",
    r"(?i)\.asp$",
    r"(?i)\.jsp$",
    r"(?i)\.cgi$",
    r"(?i)/wsman",
    r"(?i)/powershell"
]
EXECUTABLE_MIME_TYPES = [
    "application/x-msdownload",
    "application/x-executable",
    "application/octet-stream",
    "application/x-dosexec",
    "application/x-msdos-program",
    "application/x-ms-dos-executable",
    "application/x-msi"
]
SCAN_THRESHOLD_PORTS = 10
SCAN_THRESHOLD_HOSTS = 10
SCAN_TIME_WINDOW = 60
SSL_BLACKLIST_CA_FILE = "ssl_blacklist_ca.csv"
IP_BLACKLIST_FILE = "ip_blacklist.txt"
DOMAIN_BLACKLIST_FILE = "domain_blacklist.txt"

# URLs pour les API de Threat Intelligence
VTOTAL_API_URL = "https://www.virustotal.com/api/v3/"
ALIENVAULT_OTX_URL = "https://otx.alienvault.com/api/v1/indicators/"
MISP_API_URL = "https://your-misp-instance.com/attributes/restSearch/"


class PcapAnalyzer:
    """Classe principale optimisée pour l'analyse des fichiers PCAP."""

    def __init__(self, pcap_file: str, output_dir: str = "results",
                 api_keys: Dict[str, str] = None, sample_rate: float = 1.0,
                 deep_inspection: bool = False, max_workers: int = None) -> None:
        self.pcap_file = pcap_file
        self.output_dir = output_dir
        self.api_keys = api_keys or {}
        self.sample_rate = sample_rate
        self.deep_inspection = deep_inspection
        self.max_workers = max_workers or min(32, os.cpu_count() + 4)  # Optimisation du nombre de workers

        os.makedirs(self.output_dir, exist_ok=True)

        # Résultats d'analyse
        self.ssl_certs = []
        self.http_requests = []
        self.dns_queries = []
        self.scan_activities = []
        self.smb_activities = []
        self.tls_nonstandard = []
        self.suspicious_activities = []
        self.potential_iocs = set()
        self.extracted_files = []
        self.file_hashes = {}
        self.malicious_files = []

        # Créer un répertoire pour stocker les fichiers extraits
        self.files_dir = os.path.join(self.output_dir, "extracted_files")
        os.makedirs(self.files_dir, exist_ok=True)

        # Statistiques
        self.stats = {
            "total_packets": 0,
            "analyzed_packets": 0,
            "ssl_connections": 0,
            "http_requests": 0,
            "dns_queries": 0,
            "smb_connections": 0,
            "kerberos_exchanges": 0,
            "suspicious_certs": 0,
            "suspicious_domains": 0,
            "scan_activities": 0,
            "extracted_files": 0,
            "malicious_files": 0
        }

        # Données agrégées
        self.ip_connections = defaultdict(set)
        self.ip_ports = defaultdict(set)
        self.domain_queries = defaultdict(int)
        self.user_agents = Counter()
        self.mime_types = Counter()
        self.cert_issuers = Counter()
        self.http_methods = Counter()
        self.ssl_versions = Counter()
        self.dns_record_types = Counter()

        # Cache pour éviter les recalculs (optimisation DNS)
        self.domain_entropy_cache = {}
        self.analyzed_domains = set()
        
        # Verrous pour éviter les conditions de concurrence lors de l'accès aux données partagées
        self._lock = defaultdict(lambda: threading.Lock())

        # Chargement des bases de données de blacklist
        self._load_blacklists()

        # Définition des patterns de détection
        self._define_detection_patterns()

    def _load_blacklists(self) -> None:
        """Charge les fichiers de blacklist selon leurs formats."""
        # Implémentation inchangée
        self.ssl_blacklist = self._load_blacklist_file(SSL_BLACKLIST_CA_FILE)
        self.ip_blacklist = self._load_blacklist_file(IP_BLACKLIST_FILE)
        self.domain_blacklist = self._load_blacklist_file(DOMAIN_BLACKLIST_FILE)

    def _load_blacklist_file(self, filename: str) -> Dict:
        """Charge un fichier de blacklist selon son format."""
        result = {"domains": [], "ips": [], "issuers": [], "reasons": {}}

        if not os.path.exists(filename):
            return result

        # Déterminer le format en fonction de l'extension
        _, ext = os.path.splitext(filename)
        ext = ext.lower()

        try:
            if ext == '.json':
                # Charger comme JSON
                with open(filename, 'r') as f:
                    data = json.load(f)
                    return data

            elif ext == '.csv':
                # Charger comme CSV
                with open(filename, 'r', newline='') as f:
                    reader = csv.reader(f)
                    header = next(reader, None)  # Lire l'en-tête si présent

                    if not header:
                        return result

                    # Déterminer les colonnes pertinentes
                    value_idx = 0
                    reason_idx = None

                    for i, col in enumerate(header):
                        col = col.lower()
                        if "value" in col or "domain" in col or "ip" in col or "issuer" in col:
                            value_idx = i
                        if "reason" in col or "description" in col:
                            reason_idx = i

                    # Déterminer le type de blacklist en fonction du nom de fichier
                    entry_type = "domains"
                    if "ssl" in filename.lower() or "cert" in filename.lower():
                        entry_type = "issuers"
                    elif "ip" in filename.lower():
                        entry_type = "ips"

                    # Lire les entrées
                    for row in reader:
                        if len(row) > value_idx:
                            value = row[value_idx].strip()
                            if value:
                                result[entry_type].append(value)

                                # Ajouter la raison si disponible
                                if reason_idx is not None and len(row) > reason_idx:
                                    reason = row[reason_idx].strip()
                                    if reason:
                                        result["reasons"][value] = reason

                    return result

            else:  # .txt ou autre
                # Charger comme liste simple, une entrée par ligne
                with open(filename, 'r') as f:
                    lines = f.readlines()

                    # Déterminer le type de blacklist en fonction du nom de fichier
                    entry_type = "domains"
                    if "ssl" in filename.lower() or "cert" in filename.lower():
                        entry_type = "issuers"
                    elif "ip" in filename.lower():
                        entry_type = "ips"

                    for line in lines:
                        line = line.strip()
                        if line and not line.startswith("#"):  # Ignorer les commentaires
                            result[entry_type].append(line)

                    return result

        except Exception:
            return result

    def _apply_microsoft_filter(self, packet):
        """
        Filtre les paquets pour exclure les domaines Microsoft/Windows et protocoles de découverte.
        Optimisé pour minimiser les exceptions.
        """
        try:
            # Filtrer les protocoles de découverte Windows
            if (hasattr(packet, 'llmnr') or hasattr(packet, 'mdns') or
                hasattr(packet, 'nbns') or hasattr(packet, 'netbios') or
                hasattr(packet, 'dcerpc') or hasattr(packet, 'kerberos') or
                hasattr(packet, 'smb') or hasattr(packet, 'smb2')):
                return False

            # Filtrer le protocole WS-Discovery (UDP port 3702)
            if (hasattr(packet, 'udp') and
                (packet.udp.srcport == '3702' or packet.udp.dstport == '3702')):
                return False

            # Filtrer les requêtes DNS vers les domaines Microsoft
            if hasattr(packet, 'dns') and hasattr(packet.dns, 'qry_name'):
                domain = packet.dns.qry_name.lower()
                filter_domains = [
                    '.microsoft.com', '.windows.com', '.msn.com', '.live.com',
                    '.bing.com', '.office.com', '.skype.com', '.azure.com',
                    '.outlook.com', '.svc.ms', '.msftncsi.com', '.apple.com',
                    '.icloud.com', '.windows.net', '.msedge.net', '.windowsupdate.com',
                    '.windowsazure.com', '.msftauth.net', '.twitter.com', '.oracle.com',
                    '.microsoftonline.com', 'virtualearth.net', '.digicert.com',
                    '.intel.com', 'googleapis.com'
                ]

                for filter_domain in filter_domains:
                    if domain.endswith(filter_domain):
                        return False

            return True

        except Exception:
            return True  # En cas d'erreur, on accepte le paquet par défaut

    def _define_detection_patterns(self) -> None:
        """Définit des patterns regex pour la détection de comportements suspects."""
        # Patterns pour la détection de C2 HTTP
        self.http_c2_patterns = [
            re.compile(r"X-Status:\s*", re.IGNORECASE),
            re.compile(r"X-ID:\s*", re.IGNORECASE),
            re.compile(r"/gate\.php", re.IGNORECASE),
            re.compile(r"/panel\.php", re.IGNORECASE),
            re.compile(r"/admin\.php", re.IGNORECASE),
            re.compile(r"[a-zA-Z0-9+/]{30,}={0,2}")  # Base64
        ]

        # Patterns pour la détection de tunneling DNS
        self.dns_tunneling_patterns = [
            re.compile(r"[a-zA-Z0-9]{30,}\.[a-z0-9]+\.[a-z]+"),
            re.compile(r"[a-zA-Z0-9+/]{20,}={0,2}\.[a-z]+\.[a-z]+")
        ]

    def analyze(self) -> Dict[str, Any]:
        """Fonction principale d'analyse du fichier PCAP, optimisée avec multithreading."""
        start_time = time.time()

        # Vérifier si le fichier existe
        if not os.path.exists(self.pcap_file):
            return {"error": f"Le fichier {self.pcap_file} n'existe pas"}

        # Extraction des métadonnées PCAP
        file_size = os.path.getsize(self.pcap_file) / (1024 * 1024)  # taille en Mo
        
        # Ajustement dynamique du taux d'échantillonnage en fonction de la taille
        if self.sample_rate == 1.0 and file_size > 500:  # Si fichier > 500 Mo et aucun échantillonnage spécifié
            self.sample_rate = 0.5  # 50% d'échantillonnage par défaut pour les gros fichiers
        
        try:
            # Ouverture du fichier PCAP avec pyshark
            capture = pyshark.FileCapture(self.pcap_file)

            # Déterminer le nombre total de paquets pour la barre de progression
            try:
                total_packets = sum(1 for _ in pyshark.FileCapture(self.pcap_file, only_summaries=True))
            except Exception:
                total_packets = 0

            # Analyser les paquets avec échantillonnage et multithreading
            self._analyze_packets_parallel(capture, total_packets)

            # Fermer le fichier PCAP
            capture.close()

            # Corrélation des données et identification des comportements suspects
            self._correlate_data()

            # Enrichissement avec Threat Intelligence si des clés API sont disponibles
            if self.api_keys:
                self._enrich_with_threat_intelligence_parallel()

            # Générer des rapports
            self._generate_reports()

            end_time = time.time()
            analysis_time = end_time - start_time

            # Retourner les résultats
            return self._prepare_results(analysis_time)

        except Exception as e:
            return {"error": str(e)}

    def _analyze_packets_parallel(self, capture, total_packets: int) -> None:
        """
        Analyse les paquets en parallèle, avec partitionnement en lots.
        
        Cette méthode est une version optimisée qui divise le travail en lots
        et traite chaque lot dans un thread séparé.
        """
        try:
            import threading  # Import dans la fonction pour éviter les problèmes potentiels
            
            # Dictionnaire partagé pour le suivi des scans
            scan_tracking = {}
            scan_lock = threading.Lock()
            
            # Fonction pour traiter un lot de paquets
            def process_batch(packet_batch):
                batch_count = 0
                for packet in packet_batch:
                    # Comptage global (avec verrou)
                    with self._lock['stats']:
                        self.stats["total_packets"] += 1

                    # Appliquer le filtre Microsoft
                    if not self._apply_microsoft_filter(packet):
                        continue

                    # Échantillonnage
                    if random.random() > self.sample_rate:
                        continue

                    # Comptage des paquets analysés
                    with self._lock['stats']:
                        self.stats["analyzed_packets"] += 1
                        batch_count += 1

                    try:
                        # Analyse en fonction des protocoles
                        if hasattr(packet, 'tls') and hasattr(packet.tls, 'handshake'):
                            self._analyze_tls_handshake(packet)

                        if hasattr(packet, 'http'):
                            self._analyze_http_traffic(packet)

                            # Extraction de fichiers HTTP
                            if hasattr(packet.http, 'response'):
                                success, file_data, file_info = self._extract_file_from_packet(packet, 'http')
                                if success and file_data and file_info:
                                    self._process_extracted_file(file_data, file_info)
                                    with self._lock['stats']:
                                        self.stats["extracted_files"] += 1

                        if hasattr(packet, 'dns'):
                            self._analyze_dns_traffic(packet)

                        if hasattr(packet, 'smb') or hasattr(packet, 'smb2'):
                            self._analyze_smb_traffic(packet)

                            # Extraction de fichiers SMB
                            success, file_data, file_info = self._extract_file_from_packet(packet, 'smb')
                            if success and file_data and file_info:
                                self._process_extracted_file(file_data, file_info)
                                with self._lock['stats']:
                                    self.stats["extracted_files"] += 1

                        # Extraction de fichiers FTP
                        if hasattr(packet, 'ftp-data'):
                            success, file_data, file_info = self._extract_file_from_packet(packet, 'ftp-data')
                            if success and file_data and file_info:
                                self._process_extracted_file(file_data, file_info)
                                with self._lock['stats']:
                                    self.stats["extracted_files"] += 1

                        # Détection de scans (avec verrou pour éviter les conditions de concurrence)
                        if hasattr(packet, 'tcp'):
                            with scan_lock:
                                self._detect_port_scans(packet, scan_tracking)

                    except Exception:
                        continue
                
                return batch_count

            # Diviser la capture en lots pour le traitement parallèle
            batch_size = 1000  # Taille optimale pour le batch processing
            if total_packets > 0:
                batches = (total_packets + batch_size - 1) // batch_size
            else:
                batches = 100  # Valeur arbitraire si on ne connaît pas la taille
            
            with tqdm(total=total_packets, desc="Analyse des paquets", unit="pkt") as pbar:
                # Créer un pool de threads pour le traitement parallèle
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = []
                    current_batch = []
                    batch_count = 0
                    
                    # Parcourir les paquets et soumettre des lots au pool
                    for packet in capture:
                        current_batch.append(packet)
                        
                        if len(current_batch) >= batch_size:
                            # Soumettre le lot actuel et créer un nouveau lot
                            futures.append(executor.submit(process_batch, current_batch))
                            current_batch = []
                    
                    # Soumettre le dernier lot s'il y a des paquets restants
                    if current_batch:
                        futures.append(executor.submit(process_batch, current_batch))
                    
                    # Attendre tous les résultats et mettre à jour la barre de progression
                    for future in concurrent.futures.as_completed(futures):
                        try:
                            batch_processed = future.result()
                            pbar.update(batch_size)  # Approximation pour la barre de progression
                        except Exception:
                            pass
        
        except Exception as e:
            print(f"Erreur lors de l'analyse parallèle des paquets: {e}")

    def _analyze_tls_handshake(self, packet) -> None:
        """Analyse les handshakes TLS pour identifier les certificats suspects."""
        try:
            # Extraction des informations TLS de base
            src_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"
            dst_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"
            src_port = packet[packet.transport_layer].srcport
            dst_port = packet[packet.transport_layer].dstport
            timestamp = float(packet.sniff_timestamp)

            # Compter les connexions SSL/TLS (thread-safe)
            with self._lock['stats']:
                self.stats["ssl_connections"] += 1

            # Vérifier si TLS est utilisé sur un port non standard
            server_port = int(dst_port)
            if server_port not in DEFAULT_SSL_PORTS:
                with self._lock['tls']:
                    self.tls_nonstandard.append({
                        "timestamp": timestamp,
                        "src_ip": src_ip,
                        "dst_ip": dst_ip,
                        "port": server_port,
                        "severity": "medium",
                        "description": f"TLS sur port non standard {server_port}"
                    })

            # Extraire et analyser les certificats si disponibles
            if hasattr(packet.tls, 'handshake_certificate'):
                try:
                    # Extraire le certificat du paquet
                    cert_data = {
                        "timestamp": timestamp,
                        "src_ip": src_ip,
                        "dst_ip": dst_ip,
                        "port": server_port
                    }

                    # Vérifier si des champs de certificat sont disponibles
                    if hasattr(packet.tls, 'handshake_certificate_issuer'):
                        cert_data["issuer"] = packet.tls.handshake_certificate_issuer
                        with self._lock['cert_issuers']:
                            self.cert_issuers[cert_data["issuer"]] += 1

                    if hasattr(packet.tls, 'handshake_certificate_subject'):
                        cert_data["subject"] = packet.tls.handshake_certificate_subject

                    # Vérifier si c'est un certificat auto-signé
                    is_self_signed = False
                    if "issuer" in cert_data and "subject" in cert_data:
                        is_self_signed = cert_data["issuer"] == cert_data["subject"]
                        cert_data["self_signed"] = is_self_signed

                    # Vérifier si émis par une CA dans la blacklist
                    is_blacklisted = False
                    if "issuer" in cert_data:
                        for blacklisted_ca in self.ssl_blacklist.get("issuers", []):
                            if blacklisted_ca in cert_data["issuer"]:
                                is_blacklisted = True
                                cert_data["blacklisted"] = True
                                cert_data["blacklist_reason"] = self.ssl_blacklist.get(
                                    "reasons", {}).get(blacklisted_ca, "CA suspecte")
                                break

                    # Ajouter à la liste des certificats (thread-safe)
                    with self._lock['ssl_certs']:
                        self.ssl_certs.append(cert_data)

                    # Marquer comme suspect si auto-signé ou blacklisté
                    if is_self_signed or is_blacklisted:
                        with self._lock['stats']:
                            self.stats["suspicious_certs"] += 1
                        
                        with self._lock['suspicious']:
                            self.suspicious_activities.append({
                                "timestamp": timestamp,
                                "type": "SSL_SUSPICIOUS_CERT",
                                "src_ip": src_ip,
                                "dst_ip": dst_ip,
                                "port": server_port,
                                "details": cert_data,
                                "severity": "high" if is_blacklisted else "medium",
                                "mitre_technique": "T1573.001"  # Encrypted Channel: Symmetric Cryptography
                            })

                        # Ajouter aux IOCs potentiels (thread-safe)
                        with self._lock['iocs']:
                            self.potential_iocs.add(f"ip:{dst_ip}")

                except Exception:
                    pass

            # Version SSL/TLS
            if hasattr(packet.tls, 'handshake_version'):
                with self._lock['ssl_versions']:
                    self.ssl_versions[packet.tls.handshake_version] += 1

        except Exception:
            pass

    def _analyze_http_traffic(self, packet) -> None:
        """Analyse le trafic HTTP pour identifier les comportements suspects."""
        try:
            # Extraction des informations HTTP de base
            src_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"
            dst_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"
            timestamp = float(packet.sniff_timestamp)

            # Compter les requêtes HTTP (thread-safe)
            with self._lock['stats']:
                self.stats["http_requests"] += 1

            # Extraire les informations de la requête
            if hasattr(packet.http, 'request'):
                try:
                    request_method = packet.http.request_method
                    with self._lock['http_methods']:
                        self.http_methods[request_method] += 1

                    request_uri = packet.http.request_uri if hasattr(packet.http, 'request_uri') else ""
                    request_host = packet.http.host if hasattr(packet.http, 'host') else ""
                    user_agent = packet.http.user_agent if hasattr(packet.http, 'user_agent') else ""

                    # Enregistrer l'user agent
                    if user_agent:
                        with self._lock['user_agents']:
                            self.user_agents[user_agent] += 1

                    # Créer un objet représentant la requête
                    http_req = {
                        "timestamp": timestamp,
                        "src_ip": src_ip,
                        "dst_ip": dst_ip,
                        "method": request_method,
                        "host": request_host,
                        "uri": request_uri,
                        "user_agent": user_agent
                    }

                    # Ajouter à la liste des requêtes HTTP (thread-safe)
                    with self._lock['http_requests']:
                        self.http_requests.append(http_req)

                    # Détecter les User-Agents suspects
                    for pattern in SUSPICIOUS_UA_PATTERNS:
                        if re.search(pattern, user_agent):
                            with self._lock['suspicious']:
                                self.suspicious_activities.append({
                                    "timestamp": timestamp,
                                    "type": "HTTP_SUSPICIOUS_UA",
                                    "src_ip": src_ip,
                                    "dst_ip": dst_ip,
                                    "user_agent": user_agent,
                                    "details": http_req,
                                    "severity": "medium",
                                    "mitre_technique": "T1071.001"  # Application Layer Protocol: Web Protocols
                                })
                            break

                    # Détecter les chemins suspects dans l'URI
                    for pattern in HTTP_SUSPICIOUS_PATHS:
                        if re.search(pattern, request_uri):
                            with self._lock['suspicious']:
                                self.suspicious_activities.append({
                                    "timestamp": timestamp,
                                    "type": "HTTP_SUSPICIOUS_PATH",
                                    "src_ip": src_ip,
                                    "dst_ip": dst_ip,
                                    "uri": request_uri,
                                    "details": http_req,
                                    "severity": "medium",
                                    "mitre_technique": "T1190"  # Exploit Public-Facing Application
                                })
                            break

                except Exception:
                    pass

            # Extraire les informations de la réponse
            if hasattr(packet.http, 'response'):
                try:
                    content_type = packet.http.content_type if hasattr(packet.http, 'content_type') else ""
                    with self._lock['mime_types']:
                        self.mime_types[content_type] += 1

                    # Détecter les téléchargements d'exécutables
                    for exe_mime in EXECUTABLE_MIME_TYPES:
                        if exe_mime in content_type:
                            with self._lock['suspicious']:
                                self.suspicious_activities.append({
                                    "timestamp": timestamp,
                                    "type": "HTTP_EXECUTABLE_DOWNLOAD",
                                    "src_ip": dst_ip,  # Inversion car réponse
                                    "dst_ip": src_ip,  # Inversion car réponse
                                    "content_type": content_type,
                                    "severity": "high",
                                    "mitre_technique": "T1105"  # Ingress Tool Transfer
                                })
                            break

                except Exception:
                    pass

        except Exception:
            pass

    def _analyze_dns_traffic(self, packet) -> None:
        """
        Analyse le trafic DNS pour identifier les comportements suspects.
        Optimisée avec un cache pour éviter les recalculs.
        """
        try:
            # Extraction des informations DNS de base
            src_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"
            dst_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"
            timestamp = float(packet.sniff_timestamp)

            # Compter les requêtes DNS
            with self._lock['stats']:
                self.stats["dns_queries"] += 1

            # Extraire les informations de la requête
            if hasattr(packet.dns, 'qry_name'):
                query_name = packet.dns.qry_name
                query_type = packet.dns.qry_type

                # Vérifier si le domaine a déjà été analysé (optimisation avec cache)
                with self._lock['analyzed_domains']:
                    if query_name in self.analyzed_domains:
                        return
                    self.analyzed_domains.add(query_name)

                # Comptage des types de requêtes DNS
                with self._lock['dns_record_types']:
                    self.dns_record_types[query_type] += 1

                # Comptage des domaines
                with self._lock['domain_queries']:
                    self.domain_queries[query_name] += 1

                # Créer un objet représentant la requête
                dns_query = {
                    "timestamp": timestamp,
                    "src_ip": src_ip,
                    "dst_ip": dst_ip,
                    "query": query_name,
                    "type": query_type
                }

                # Calculer l'entropie du nom de domaine (avec cache pour éviter les recalculs)
                with self._lock['domain_entropy_cache']:
                    if query_name in self.domain_entropy_cache:
                        domain_entropy = self.domain_entropy_cache[query_name]
                    else:
                        domain_entropy = self._calculate_entropy(query_name)
                        self.domain_entropy_cache[query_name] = domain_entropy

                dns_query["entropy"] = domain_entropy

                # Ajouter à la liste des requêtes DNS
                with self._lock['dns_queries']:
                    self.dns_queries.append(dns_query)

                # Vérifier si le domaine est dans une blacklist
                domain_blacklisted = False
                for blacklisted_domain in self.domain_blacklist.get("domains", []):
                    if blacklisted_domain in query_name:
                        domain_blacklisted = True
                        dns_query["blacklisted"] = True
                        dns_query["blacklist_reason"] = self.domain_blacklist.get(
                            "reasons", {}).get(blacklisted_domain, "Domaine suspect")
                        break

                # Compter les niveaux de sous-domaines
                subdomain_count = query_name.count('.')

                # Détecter les domaines à forte entropie (possible DGA)
                if domain_entropy > SUSPICIOUS_DOMAINS_ENTROPY_THRESHOLD:
                    with self._lock['stats']:
                        self.stats["suspicious_domains"] += 1
                    with self._lock['suspicious']:
                        self.suspicious_activities.append({
                            "timestamp": timestamp,
                            "type": "DNS_HIGH_ENTROPY",
                            "src_ip": src_ip,
                            "dst_ip": dst_ip,
                            "query": query_name,
                            "entropy": domain_entropy,
                            "details": dns_query,
                            "severity": "medium",
                            "mitre_technique": "T1568.002"  # Dynamic Resolution: Domain Generation Algorithms
                        })

                # Détecter les potentiels tunneling DNS
                if (domain_entropy > DNS_TUNNELING_ENTROPY_THRESHOLD and
                    subdomain_count >= DNS_SUBDOMAIN_DEPTH_THRESHOLD):
                    with self._lock['suspicious']:
                        self.suspicious_activities.append({
                            "timestamp": timestamp,
                            "type": "DNS_TUNNELING",
                            "src_ip": src_ip,
                            "dst_ip": dst_ip,
                            "query": query_name,
                            "entropy": domain_entropy,
                            "subdomains": subdomain_count,
                            "details": dns_query,
                            "severity": "high",
                            "mitre_technique": "T1071.004"  # Application Layer Protocol: DNS
                        })

                # Domaine blacklisté
                if domain_blacklisted:
                    with self._lock['suspicious']:
                        self.suspicious_activities.append({
                            "timestamp": timestamp,
                            "type": "DNS_MALICIOUS_DOMAIN",
                            "src_ip": src_ip,
                            "dst_ip": dst_ip,
                            "query": query_name,
                            "details": dns_query,
                            "severity": "high",
                            "mitre_technique": "T1071.004"  # Application Layer Protocol: DNS
                        })

                    # Ajouter aux IOCs potentiels
                    with self._lock['iocs']:
                        self.potential_iocs.add(f"domain:{query_name}")

        except Exception:
            pass

    def _analyze_smb_traffic(self, packet) -> None:
        """Analyse le trafic SMB/CIFS pour identifier les comportements suspects."""
        try:
            # Extraction des informations SMB de base
            src_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"
            dst_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"
            timestamp = float(packet.sniff_timestamp)

            # Compter les connexions SMB
            with self._lock['stats']:
                self.stats["smb_connections"] += 1

            # Créer un objet représentant l'activité SMB
            smb_activity = {
                "timestamp": timestamp,
                "src_ip": src_ip,
                "dst_ip": dst_ip
            }

            # Déterminer la version SMB
            if hasattr(packet, 'smb2'):
                smb_activity["version"] = "SMB2"

                # Extraire le command_id SMB2
                if hasattr(packet.smb2, 'cmd'):
                    smb_activity["command"] = packet.smb2.cmd
            else:
                smb_activity["version"] = "SMB1"

                # Extraire le command SMB1
                if hasattr(packet.smb, 'cmd'):
                    smb_activity["command"] = packet.smb.cmd

            # Ajouter à la liste des activités SMB (thread-safe)
            with self._lock['smb_activities']:
                self.smb_activities.append(smb_activity)

            # Mettre à jour le suivi des connexions (thread-safe)
            with self._lock['ip_connections']:
                self.ip_connections[src_ip].add(dst_ip)

            # Détecter les potentiels mouvements latéraux
            with self._lock['ip_connections']:
                if len(self.ip_connections[src_ip]) >= 3:  # Seuil arbitraire
                    # Vérifier si les commandes sont suspectes
                    suspicious_cmds = {"tree_connect", "create", "write", "transaction",
                                      "transaction2", "nt_create_andx"}

                    if ("command" in smb_activity and
                        smb_activity["command"].lower() in suspicious_cmds):

                        with self._lock['suspicious']:
                            self.suspicious_activities.append({
                                "timestamp": timestamp,
                                "type": "SMB_LATERAL_MOVEMENT",
                                "src_ip": src_ip,
                                "connections": list(self.ip_connections[src_ip]),
                                "command": smb_activity.get("command"),
                                "details": smb_activity,
                                "severity": "high",
                                "mitre_technique": "T1021.002"  # Remote Services: SMB/Windows Admin Shares
                            })

        except Exception:
            pass

    def _detect_port_scans(self, packet, scan_tracking: Dict) -> None:
        """Détecte les activités de scan de ports."""
        try:
            # Extraction des informations TCP de base
            src_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"
            dst_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"
            src_port = packet.tcp.srcport
            dst_port = packet.tcp.dstport
            timestamp = float(packet.sniff_timestamp)
            flags = packet.tcp.flags

            # Créer une entrée pour l'IP source si elle n'existe pas
            if src_ip not in scan_tracking:
                scan_tracking[src_ip] = {
                    "ports": set(),
                    "hosts": set(),
                    "start_time": timestamp,
                    "last_time": timestamp,
                    "syn_count": 0,
                    "ack_count": 0,
                    "rst_count": 0
                }

            # Mettre à jour le suivi
            scan_tracking[src_ip]["last_time"] = timestamp
            scan_tracking[src_ip]["hosts"].add(dst_ip)
            scan_tracking[src_ip]["ports"].add(dst_port)

            # Compter les types de flags
            if "S" in flags and "A" not in flags:  # SYN sans ACK
                scan_tracking[src_ip]["syn_count"] += 1
            if "A" in flags:
                scan_tracking[src_ip]["ack_count"] += 1
            if "R" in flags:
                scan_tracking[src_ip]["rst_count"] += 1

            # Vérifier les conditions de scan
            current_track = scan_tracking[src_ip]
            time_window = timestamp - current_track["start_time"]

            # Conditions de détection de scan
            is_scan = False
            scan_type = None

            # Scan de ports (nombreux ports sur un même hôte)
            if (len(current_track["ports"]) >= SCAN_THRESHOLD_PORTS and
                len(current_track["hosts"]) < 3 and
                time_window <= SCAN_TIME_WINDOW):
                is_scan = True
                scan_type = "PORT_SCAN"

            # Scan d'hôtes (nombreux hôtes sur un même port)
            elif (len(current_track["hosts"]) >= SCAN_THRESHOLD_HOSTS and
                  len(current_track["ports"]) < 3 and
                  time_window <= SCAN_TIME_WINDOW):
                is_scan = True
                scan_type = "HOST_SCAN"

            # Scan mixte (nombreux hôtes et ports)
            elif (len(current_track["hosts"]) >= SCAN_THRESHOLD_HOSTS and
                  len(current_track["ports"]) >= SCAN_THRESHOLD_PORTS and
                  time_window <= SCAN_TIME_WINDOW):
                is_scan = True
                scan_type = "NETWORK_SCAN"

            # Si un scan est détecté et qu'il n'a pas déjà été rapporté
            if is_scan:
                with self._lock['scan_activities']:
                    # Vérifier si ce scan n'a pas déjà été détecté
                    if any(a["type"] == scan_type and a["src_ip"] == src_ip
                          for a in self.scan_activities):
                        return

                # Déterminer le type spécifique de scan basé sur les flags
                specific_scan_type = scan_type
                if current_track["syn_count"] > 0 and current_track["ack_count"] == 0:
                    specific_scan_type += "_SYN"
                elif current_track["ack_count"] > 0 and current_track["syn_count"] == 0:
                    specific_scan_type += "_ACK"
                elif current_track["rst_count"] > 0:
                    specific_scan_type += "_RST"

                # Créer une entrée pour le scan
                scan_activity = {
                    "timestamp": timestamp,
                    "type": specific_scan_type,
                    "src_ip": src_ip,
                    "hosts_scanned": list(current_track["hosts"]),
                    "ports_scanned": list(current_track["ports"]),
                    "duration": time_window,
                    "syn_count": current_track["syn_count"],
                    "ack_count": current_track["ack_count"],
                    "rst_count": current_track["rst_count"],
                    "severity": "medium",
                    "mitre_technique": "T1046"  # Network Service Scanning
                }

                # Ajouter à la liste des activités de scan
                with self._lock['scan_activities']:
                    self.scan_activities.append(scan_activity)
                
                with self._lock['stats']:
                    self.stats["scan_activities"] += 1

                # Ajouter aux activités suspectes
                with self._lock['suspicious']:
                    self.suspicious_activities.append({
                        "timestamp": timestamp,
                        "type": specific_scan_type,
                        "src_ip": src_ip,
                        "details": scan_activity,
                        "severity": "medium",
                        "mitre_technique": "T1046"  # Network Service Scanning
                    })

            # Nettoyage des entrées trop anciennes (plus de 5 minutes)
            for ip in list(scan_tracking.keys()):
                if timestamp - scan_tracking[ip]["last_time"] > 300:
                    del scan_tracking[ip]

        except Exception:
            pass

    def _extract_file_from_packet(self, packet, protocol):
        """Extrait un fichier d'un paquet selon le protocole."""
        try:
            if protocol == 'http' and hasattr(packet, 'http'):
                # Extraction HTTP
                return self._extract_http_file(packet)
            elif (protocol == 'smb' and
                  (hasattr(packet, 'smb') or hasattr(packet, 'smb2'))):
                # Extraction SMB
                return self._extract_smb_file(packet)
            elif protocol == 'ftp-data' and hasattr(packet, 'ftp-data'):
                # Extraction FTP
                return self._extract_ftp_file(packet)

            return False, None, None

        except Exception:
            return False, None, None

    def _extract_http_file(self, packet):
        """Extrait un fichier d'une réponse HTTP."""
        try:
            # Vérifier s'il s'agit d'une réponse HTTP avec du contenu
            if (hasattr(packet.http, 'response') and
                hasattr(packet.http, 'file_data')):

                # Obtenir les métadonnées essentielles
                content_type = packet.http.content_type if hasattr(packet.http, 'content_type') else "application/octet-stream"
                content_disposition = packet.http.content_disposition if hasattr(packet.http, 'content_disposition') else ""

                # Essayer d'extraire le nom de fichier
                filename = None
                if content_disposition:
                    match = re.search(r'filename=["\']?([^"\';\n]+)', content_disposition)
                    if match:
                        filename = match.group(1)

                # Si pas de nom de fichier dans Content-Disposition, essayer d'extraire de l'URI
                if not filename and hasattr(packet.http, 'request_uri'):
                    uri_path = packet.http.request_uri
                    uri_filename = os.path.basename(uri_path)
                    if '.' in uri_filename and len(uri_filename) < 255:
                        filename = uri_filename

                # Si toujours pas de nom de fichier, générer un nom basé sur l'horodatage
                if not filename:
                    timestamp = packet.sniff_timestamp if hasattr(packet, 'sniff_timestamp') else time.time()
                    ext = self._get_extension_from_content_type(content_type)
                    filename = f"http_file_{int(float(timestamp))}{ext}"

                # Obtenir les données du fichier
                file_data = bytes.fromhex(packet.http.file_data.replace(':', ''))

                # Créer les métadonnées du fichier
                src_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"
                dst_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"

                file_info = {
                    "timestamp": float(packet.sniff_timestamp) if hasattr(packet, 'sniff_timestamp') else time.time(),
                    "protocol": "HTTP",
                    "src_ip": src_ip,
                    "dst_ip": dst_ip,
                    "filename": filename,
                    "size": len(file_data),
                    "content_type": content_type,
                    "uri": packet.http.request_uri if hasattr(packet.http, 'request_uri') else None
                }

                return True, file_data, file_info

            return False, None, None

        except Exception:
            return False, None, None

    def _extract_smb_file(self, packet):
        """Extrait un fichier d'une session SMB."""
        try:
            # Vérifier s'il s'agit d'une réponse SMB avec du contenu de fichier
            is_smb2 = hasattr(packet, 'smb2')
            smb_layer = packet.smb2 if is_smb2 else packet.smb

            # Vérifier si le paquet contient des données de fichier
            if ((is_smb2 and hasattr(smb_layer, 'cmd') and smb_layer.cmd == '8') or  # READ pour SMB2
                (not is_smb2 and hasattr(smb_layer, 'cmd') and smb_layer.cmd == '37')):  # READ_ANDX pour SMB1

                # Vérifier la présence de données
                if (hasattr(packet, 'data') and
                    hasattr(packet.data, 'data') and
                    len(packet.data.data) > 0):

                    # Obtenir des identifiants pour cette session SMB
                    session_id = smb_layer.sesid if hasattr(smb_layer, 'sesid') else "Unknown"
                    tree_id = smb_layer.tid if hasattr(smb_layer, 'tid') else "Unknown"
                    file_id = "Unknown"

                    if is_smb2 and hasattr(smb_layer, 'fid'):
                        file_id = smb_layer.fid

                    # Générer un nom de fichier
                    filename = f"smb_file_{session_id}_{tree_id}_{file_id}"

                    # Obtenir les données du fichier
                    file_data = bytes.fromhex(packet.data.data.replace(':', ''))

                    # Créer les métadonnées du fichier
                    src_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"
                    dst_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"

                    file_info = {
                        "timestamp": float(packet.sniff_timestamp) if hasattr(packet, 'sniff_timestamp') else time.time(),
                        "protocol": "SMB" + ("2" if is_smb2 else "1"),
                        "src_ip": src_ip,
                        "dst_ip": dst_ip,
                        "filename": filename,
                        "size": len(file_data),
                        "session_id": session_id,
                        "tree_id": tree_id,
                        "file_id": file_id
                    }

                    return True, file_data, file_info

            return False, None, None

        except Exception:
            return False, None, None

    def _extract_ftp_file(self, packet):
        """Extrait un fichier d'une session FTP-DATA."""
        try:
            # Vérifier la présence de données FTP
            if (hasattr(packet, 'ftp-data') and
                hasattr(packet.ftpdata, 'data')):

                # Obtenir les données du fichier
                file_data = bytes.fromhex(packet.ftpdata.data.replace(':', ''))

                # Générer un nom de fichier basé sur les ports
                src_port = packet.tcp.srcport if hasattr(packet, 'tcp') else "Unknown"
                dst_port = packet.tcp.dstport if hasattr(packet, 'tcp') else "Unknown"
                filename = f"ftp_file_{src_port}_{dst_port}_{int(time.time())}"

                # Créer les métadonnées du fichier
                src_ip = packet.ip.src if hasattr(packet, 'ip') else "Unknown"
                dst_ip = packet.ip.dst if hasattr(packet, 'ip') else "Unknown"

                file_info = {
                    "timestamp": float(packet.sniff_timestamp) if hasattr(packet, 'sniff_timestamp') else time.time(),
                    "protocol": "FTP-DATA",
                    "src_ip": src_ip,
                    "dst_ip": dst_ip,
                    "filename": filename,
                    "size": len(file_data),
                    "src_port": src_port,
                    "dst_port": dst_port
                }

                return True, file_data, file_info

            return False, None, None

        except Exception:
            return False, None, None

    @lru_cache(maxsize=1024)  # Cache optimisé pour les calculs coûteux
    def _calculate_entropy(self, s: str) -> float:
        """
        Calcule l'entropie de Shannon d'une chaîne.
        Optimisé avec un cache pour éviter les recalculs.
        """
        if not s:
            return 0.0

        # Compter les occurrences de chaque caractère
        char_count = Counter(s)

        # Calculer la probabilité de chaque caractère
        probabilities = [count / len(s) for count in char_count.values()]

        # Calculer l'entropie
        entropy = -sum(p * np.log2(p) for p in probabilities)

        return entropy

    def _calculate_file_hash(self, file_data):
        """Calcule le hash SHA-256 des données d'un fichier."""
        sha256_hash = hashlib.sha256(file_data).hexdigest()
        return sha256_hash

    def _get_extension_from_content_type(self, content_type):
        """Détermine l'extension de fichier appropriée selon le type MIME."""
        # Mapping des types MIME courants vers des extensions
        mime_to_ext = {
            "application/x-dosexec": ".exe",
            "application/x-msdownload": ".exe",
            "application/x-msdos-program": ".exe",
            "application/x-executable": ".exe",
            "application/pdf": ".pdf",
            "application/zip": ".zip",
            "application/x-rar-compressed": ".rar",
            "application/x-7z-compressed": ".7z",
            "application/msword": ".doc",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
            "application/vnd.ms-excel": ".xls",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": ".xlsx",
            "application/vnd.ms-powerpoint": ".ppt",
            "application/vnd.openxmlformats-officedocument.presentationml.presentation": ".pptx",
            "application/javascript": ".js",
            "text/javascript": ".js",
            "text/html": ".html",
            "text/css": ".css",
            "text/plain": ".txt",
            "text/xml": ".xml",
            "image/jpeg": ".jpg",
            "image/png": ".png",
            "image/gif": ".gif",
            "image/bmp": ".bmp",
            "image/svg+xml": ".svg",
            "audio/mpeg": ".mp3",
            "audio/wav": ".wav",
            "video/mp4": ".mp4",
            "video/mpeg": ".mpeg",
            "application/octet-stream": ".bin"
        }

        # Extraire le type MIME principal
        content_type = content_type.split(';')[0].strip().lower()

        # Retourner l'extension correspondante ou .bin par défaut
        return mime_to_ext.get(content_type, ".bin")

    def _save_extracted_file(self, file_data, file_info, file_hash):
        """Sauvegarde un fichier extrait dans le système de fichiers."""
        try:
            # Créer un nom de fichier sécurisé
            safe_filename = re.sub(r'[^a-zA-Z0-9_.-]', '_', file_info["filename"])

            # Limiter la longueur du nom de fichier
            if len(safe_filename) > 200:
                name, ext = os.path.splitext(safe_filename)
                safe_filename = name[:195] + ext if ext else name[:200]

            # Ajouter le hash comme préfixe pour garantir l'unicité
            full_path = os.path.join(self.files_dir, f"{file_hash[:8]}_{safe_filename}")

            # Éviter les collisions de noms de fichiers
            counter = 1
            original_path = full_path
            while os.path.exists(full_path):
                name, ext = os.path.splitext(original_path)
                full_path = f"{name}_{counter}{ext}"
                counter += 1

            # Écrire le fichier
            with open(full_path, 'wb') as f:
                f.write(file_data)

            return full_path

        except Exception:
            return None

    def _analyze_file_for_malware(self, file_hash, file_info, filepath):
        """
        Analyse un fichier extrait pour déterminer s'il est malveillant.
        Utilise uniquement la vérification via VirusTotal (known_hash).
        """
        # Initialiser les variables de retour
        is_malicious = False
        threat_info = {}

        # Vérifier dans les bases de données de Threat Intelligence si des clés API sont disponibles
        if "virustotal" in self.api_keys and file_hash:
            vtotal_info = self._check_virustotal_hash(file_hash)
            if vtotal_info:
                threat_info["virustotal"] = vtotal_info
                if vtotal_info.get("malicious_count", 0) > 5:  # Seuil arbitraire
                    is_malicious = True
                    threat_info["detection_method"] = "known_hash"
                    threat_info["confidence"] = "high"

                    # Ajouter aux IOCs (thread-safe)
                    with self._lock['iocs']:
                        self.potential_iocs.add(f"file:sha256:{file_hash}")

                    # Augmenter le compteur de fichiers malveillants (thread-safe)
                    with self._lock['stats']:
                        self.stats["malicious_files"] += 1

                    # Ajouter une activité suspecte (thread-safe)
                    with self._lock['suspicious']:
                        self.suspicious_activities.append({
                            "timestamp": file_info["timestamp"],
                            "type": "MALICIOUS_FILE_DETECTED",
                            "src_ip": file_info["src_ip"],
                            "dst_ip": file_info["dst_ip"],
                            "protocol": file_info["protocol"],
                            "filename": file_info["filename"],
                            "file_hash": file_hash,
                            "file_size": file_info["size"],
                            "threat_info": threat_info,
                            "severity": "high",
                            "mitre_technique": "T1105"  # Ingress Tool Transfer
                        })

        return is_malicious, threat_info

    def _check_virustotal_hash(self, file_hash):
        """Vérifie un hash de fichier sur VirusTotal."""
        try:
            api_key = self.api_keys.get("virustotal")
            headers = {
                'x-apikey': api_key,
                'Accept': 'application/json'
            }

            url = f"{VTOTAL_API_URL}files/{file_hash}"

            response = requests.get(url, headers=headers)

            if response.status_code == 200:
                data = response.json()
                attributes = data.get('data', {}).get('attributes', {})

                last_analysis_stats = attributes.get('last_analysis_stats', {})
                malicious_count = last_analysis_stats.get('malicious', 0)
                suspicious_count = last_analysis_stats.get('suspicious', 0)

                # Nom détecté du malware
                suggested_threat_name = "Unknown"
                if malicious_count > 0:
                    # Essayer de trouver un nom cohérent parmi les moteurs
                    last_analysis_results = attributes.get('last_analysis_results', {})
                    malware_names = [engine_result.get('result')
                                    for engine_result in last_analysis_results.values()
                                    if engine_result.get('result')]

                    if malware_names:
                        # Utiliser le nom le plus fréquent
                        name_counts = Counter(malware_names)
                        suggested_threat_name = name_counts.most_common(1)[0][0]

                return {
                    "scan_date": attributes.get('last_analysis_date'),
                    "malicious_count": malicious_count,
                    "suspicious_count": suspicious_count,
                    "total_engines": sum(last_analysis_stats.values()),
                    "threat_name": suggested_threat_name
                }

            return None

        except Exception:
            return None

    def _process_extracted_file(self, file_data, file_info):
        """Traite un fichier extrait: calcule son hash, le sauvegarde et l'analyse."""
        try:
            # Calculer le hash SHA-256
            file_hash = self._calculate_file_hash(file_data)

            # Sauvegarder dans la structure interne (thread-safe)
            file_info["sha256"] = file_hash
            file_info["extraction_time"] = time.time()
            
            with self._lock['extracted_files']:
                self.extracted_files.append(file_info)
                self.file_hashes[file_hash] = file_info

            # Sauvegarder le fichier sur disque
            filepath = self._save_extracted_file(file_data, file_info, file_hash)
            file_info["filepath"] = filepath

            # Analyser le fichier pour détecter s'il est malveillant
            is_malicious, threat_info = self._analyze_file_for_malware(file_hash, file_info, filepath)

            if is_malicious:
                file_info["malicious"] = True
                file_info["threat_info"] = threat_info
                with self._lock['malicious_files']:
                    self.malicious_files.append(file_info)

            return True

        except Exception:
            return False

    def _correlate_data(self) -> None:
        """Corrèle les différentes sources de données pour identifier des patterns d'attaque."""
        try:
            # Générer des IOCs à partir des activités suspectes
            for activity in self.suspicious_activities:
                activity_type = activity.get("type", "")

                # Générer des IOCs pour les IP sources et destinations suspectes
                if "src_ip" in activity and self._is_valid_ip(activity["src_ip"]):
                    self.potential_iocs.add(f"ip:{activity['src_ip']}")

                if "dst_ip" in activity and self._is_valid_ip(activity["dst_ip"]):
                    self.potential_iocs.add(f"ip:{activity['dst_ip']}")

                # Générer des IOCs pour les domaines suspects
                if "query" in activity:
                    self.potential_iocs.add(f"domain:{activity['query']}")

                # Générer des IOCs pour les User-Agents suspects
                if activity_type == "HTTP_SUSPICIOUS_UA" and "user_agent" in activity:
                    self.potential_iocs.add(f"useragent:{activity['user_agent']}")

                # Générer des IOCs pour les URI suspects
                if "uri" in activity:
                    self.potential_iocs.add(f"uri:{activity['uri']}")

                # Générer des IOCs pour les hashes de fichiers
                if activity_type == "MALICIOUS_FILE_DETECTED" and "file_hash" in activity:
                    self.potential_iocs.add(f"file:sha256:{activity['file_hash']}")

            # Analyse temporelle des activités suspectes
            # Regrouper les activités par fenêtres temporelles
            time_windows = {}
            for activity in self.suspicious_activities:
                # Arrondir le timestamp à la minute la plus proche
                rounded_time = int(activity["timestamp"] // 60) * 60
                if rounded_time not in time_windows:
                    time_windows[rounded_time] = []
                time_windows[rounded_time].append(activity)

            # Chercher des séquences d'événements caractéristiques d'attaques
            for time_window, activities in time_windows.items():
                # Exemple: Reconnaissance suivie de mouvement latéral
                has_scan = any(a["type"].startswith(("PORT_SCAN", "HOST_SCAN", "NETWORK_SCAN"))
                              for a in activities)
                has_lateral = any(a["type"] == "SMB_LATERAL_MOVEMENT" for a in activities)

                if has_scan and has_lateral:
                    scan_activity = next(a for a in activities
                                        if a["type"].startswith(("PORT_SCAN", "HOST_SCAN", "NETWORK_SCAN")))
                    lateral_activity = next(a for a in activities
                                           if a["type"] == "SMB_LATERAL_MOVEMENT")

                    # Si la même IP source est impliquée dans les deux
                    if scan_activity["src_ip"] == lateral_activity["src_ip"]:
                        self.suspicious_activities.append({
                            "timestamp": time_window,
                            "type": "ATTACK_PATTERN_RECONNAISSANCE_LATERAL",
                            "src_ip": scan_activity["src_ip"],
                            "activities": [scan_activity, lateral_activity],
                            "severity": "critical",
                            "mitre_tactic": "TA0008"  # Lateral Movement
                        })

                # Exemple: Scan suivi d'exploitation web
                has_web_exploit = any(a["type"] == "HTTP_SUSPICIOUS_PATH" for a in activities)

                if has_scan and has_web_exploit:
                    scan_activity = next(a for a in activities
                                        if a["type"].startswith(("PORT_SCAN", "HOST_SCAN", "NETWORK_SCAN")))
                    exploit_activity = next(a for a in activities
                                          if a["type"] == "HTTP_SUSPICIOUS_PATH")

                    self.suspicious_activities.append({
                        "timestamp": time_window,
                        "type": "ATTACK_PATTERN_SCAN_EXPLOIT",
                        "src_ip": scan_activity["src_ip"],
                        "target_ip": exploit_activity["dst_ip"],
                        "activities": [scan_activity, exploit_activity],
                        "severity": "high",
                        "mitre_tactic": "TA0001"  # Initial Access
                    })

                # Exemple: Téléchargement de fichier malveillant suivi d'activité C2
                has_malicious_file = any(a["type"] == "MALICIOUS_FILE_DETECTED" for a in activities)
                has_c2_activity = any(a["type"] == "POTENTIAL_C2_ACTIVITY" for a in activities)

                if has_malicious_file and has_c2_activity:
                    file_activity = next(a for a in activities
                                        if a["type"] == "MALICIOUS_FILE_DETECTED")
                    c2_activity = next(a for a in activities
                                      if a["type"] == "POTENTIAL_C2_ACTIVITY")

                    # Si l'IP dst du fichier est l'IP src de l'activité C2
                    if file_activity["dst_ip"] == c2_activity["src_ip"]:
                        self.suspicious_activities.append({
                            "timestamp": time_window,
                            "type": "ATTACK_PATTERN_MALWARE_C2",
                            "file_hash": file_activity.get("file_hash"),
                            "infected_host": file_activity["dst_ip"],
                            "c2_connections": c2_activity.get("out_connections", 0),
                            "activities": [file_activity, c2_activity],
                            "severity": "critical",
                            "mitre_tactic": "TA0011"  # Command and Control
                        })

            # Analyse des connexions IP pour détecter des patterns de botnet
            # Calculer le rapport entre les connexions sortantes et entrantes
            ip_in_connections = defaultdict(int)
            ip_out_connections = defaultdict(int)

            for src_ip, dst_ips in self.ip_connections.items():
                ip_out_connections[src_ip] += len(dst_ips)
                for dst_ip in dst_ips:
                    ip_in_connections[dst_ip] += 1

            # Identifier les IPs avec un rapport déséquilibré
            for ip, out_count in ip_out_connections.items():
                in_count = ip_in_connections.get(ip, 0)

                # Si beaucoup de connexions sortantes et peu entrantes (possible C2)
                if out_count > 10 and in_count < 3 and out_count / max(1, in_count) > 5:
                    self.suspicious_activities.append({
                        "timestamp": max(a["timestamp"] for a in self.suspicious_activities) if self.suspicious_activities else time.time(),
                        "type": "POTENTIAL_C2_ACTIVITY",
                        "src_ip": ip,
                        "out_connections": out_count,
                        "in_connections": in_count,
                        "ratio": out_count / max(1, in_count),
                        "severity": "high",
                        "mitre_technique": "T1071"  # Command and Control
                    })

        except Exception:
            pass

    def _enrich_with_threat_intelligence_parallel(self) -> None:
        """
        Enrichit l'analyse avec des données de Threat Intelligence, optimisé avec du threading.
        """
        try:
            # Collecter les IPs, domaines et hashes de fichiers uniques pour l'enrichissement
            unique_ips = set()
            unique_domains = set()
            unique_file_hashes = set()

            # Extraire depuis les activités suspectes
            for activity in self.suspicious_activities:
                if "src_ip" in activity and self._is_valid_ip(activity["src_ip"]):
                    unique_ips.add(activity["src_ip"])
                if "dst_ip" in activity and self._is_valid_ip(activity["dst_ip"]):
                    unique_ips.add(activity["dst_ip"])
                if "query" in activity:
                    unique_domains.add(activity["query"])
                if "file_hash" in activity:
                    unique_file_hashes.add(activity["file_hash"])

            # Extraire depuis les IOCs potentiels
            for ioc in self.potential_iocs:
                if ioc.startswith("ip:") and self._is_valid_ip(ioc[3:]):
                    unique_ips.add(ioc[3:])
                if ioc.startswith("domain:"):
                    unique_domains.add(ioc[7:])
                if ioc.startswith("file:sha256:"):
                    unique_file_hashes.add(ioc[11:])

            # Fonction pour enrichir VirusTotal en parallèle
            def process_virustotal(batch):
                results = {}
                if "virustotal" in self.api_keys:
                    for item in batch:
                        if isinstance(item, tuple) and item[0] == 'domain':
                            domain = item[1]
                            result = self._enrich_domain_virustotal(domain)
                            if result:
                                results[f'domain:{domain}'] = result
                        elif isinstance(item, tuple) and item[0] == 'hash':
                            file_hash = item[1]
                            result = self._check_virustotal_hash(file_hash)
                            if result:
                                results[f'hash:{file_hash}'] = result
                        # Attente pour respecter les limites d'API
                        time.sleep(2)
                return results

            # Préparer les lots pour le traitement parallèle
            vt_batches = []
            current_batch = []
            batch_size = 10
            
            # Ajouter les domaines aux lots
            for domain in unique_domains:
                current_batch.append(('domain', domain))
                if len(current_batch) >= batch_size:
                    vt_batches.append(current_batch)
                    current_batch = []
            
            # Ajouter les hashes aux lots
            for file_hash in unique_file_hashes:
                current_batch.append(('hash', file_hash))
                if len(current_batch) >= batch_size:
                    vt_batches.append(current_batch)
                    current_batch = []
            
            # Ajouter le dernier lot s'il y a des éléments
            if current_batch:
                vt_batches.append(current_batch)

            # Traiter les lots en parallèle
            vt_results = {}
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(process_virustotal, batch) for batch in vt_batches]
                for future in concurrent.futures.as_completed(futures):
                    batch_results = future.result()
                    vt_results.update(batch_results)

            # Mettre à jour les activités suspectes avec les résultats
            self._update_activities_with_ti(vt_results)

            # Enrichir avec AlienVault OTX si la clé API est disponible
            if "alienvault" in self.api_keys:
                self._enrich_with_alienvault_parallel(unique_ips, unique_domains)

            # Enrichir avec MISP si la clé API est disponible
            if "misp" in self.api_keys:
                self._enrich_with_misp(unique_ips, unique_domains)

        except Exception:
            pass

    def _enrich_domain_virustotal(self, domain):
        """Enrichir un domaine avec des données de VirusTotal."""
        try:
            api_key = self.api_keys.get("virustotal")
            headers = {
                'x-apikey': api_key,
                'Accept': 'application/json'
            }

            url = f"{VTOTAL_API_URL}domains/{domain}"
            response = requests.get(url, headers=headers)

            if response.status_code == 200:
                data = response.json()
                domain_data = data.get('data', {}).get('attributes', {})

                last_analysis_stats = domain_data.get('last_analysis_stats', {})
                malicious_count = last_analysis_stats.get('malicious', 0)
                suspicious_count = last_analysis_stats.get('suspicious', 0)

                return {
                    "malicious_count": malicious_count,
                    "suspicious_count": suspicious_count,
                    "categories": domain_data.get('categories', {})
                }

            return None
        except Exception:
            return None

    def _update_activities_with_ti(self, ti_results):
        """Mettre à jour les activités suspectes avec les résultats de Threat Intelligence."""
        try:
            for activity in self.suspicious_activities:
                # Mettre à jour pour les domaines
                if "query" in activity:
                    domain = activity["query"]
                    result = ti_results.get(f'domain:{domain}')
                    if result:
                        activity["ti_data"] = activity.get("ti_data", {})
                        activity["ti_data"]["virustotal"] = result
                        
                        # Ajuster la sévérité
                        if result.get("malicious_count", 0) > 5:
                            activity["severity"] = "high"
                            if result.get("malicious_count", 0) > 15:
                                activity["severity"] = "critical"
                        elif result.get("suspicious_count", 0) > 10:
                            activity["severity"] = "medium"

                # Mettre à jour pour les fichiers
                if "file_hash" in activity:
                    file_hash = activity["file_hash"]
                    result = ti_results.get(f'hash:{file_hash}')
                    if result:
                        activity["ti_data"] = activity.get("ti_data", {})
                        activity["ti_data"]["virustotal"] = result
                        
                        # Ajuster la sévérité
                        if result.get("malicious_count", 0) > 15:
                            activity["severity"] = "critical"
                        elif result.get("malicious_count", 0) > 5:
                            activity["severity"] = "high"
        except Exception:
            pass

    def _enrich_with_alienvault_parallel(self, ips: Set[str], domains: Set[str]) -> None:
        """
        Enrichit les IPs et domaines avec des données d'AlienVault OTX de manière parallèle.

        Args:
            ips: Ensemble d'adresses IP à enrichir
            domains: Ensemble de domaines à enrichir
        """
        api_key = self.api_keys.get("alienvault")
        if not api_key:
            return

        headers = {
            'X-OTX-API-KEY': api_key,
            'Accept': 'application/json'
        }

        def process_ip(ip):
            try:
                url = f"{ALIENVAULT_OTX_URL}IPv4/{ip}/general"
                response = requests.get(url, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    pulse_count = data.get('pulse_info', {}).get('count', 0)
                    pulses = data.get('pulse_info', {}).get('pulses', [])
                    
                    # Extraire les tags des pulses
                    tags = set()
                    for pulse in pulses:
                        tags.update(pulse.get('tags', []))
                    
                    return {
                        "ip": ip,
                        "pulse_count": pulse_count,
                        "tags": list(tags)
                    }
                return None
            except Exception:
                return None

        def process_domain(domain):
            try:
                url = f"{ALIENVAULT_OTX_URL}domain/{domain}/general"
                response = requests.get(url, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    pulse_count = data.get('pulse_info', {}).get('count', 0)
                    pulses = data.get('pulse_info', {}).get('pulses', [])
                    
                    # Extraire les tags des pulses
                    tags = set()
                    for pulse in pulses:
                        tags.update(pulse.get('tags', []))
                    
                    return {
                        "domain": domain,
                        "pulse_count": pulse_count,
                        "tags": list(tags)
                    }
                return None
            except Exception:
                return None

        # Traiter les IPs et domaines en parallèle
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            ip_futures = {executor.submit(process_ip, ip): ip for ip in ips}
            domain_futures = {executor.submit(process_domain, domain): domain for domain in domains}
            
            # Collecter les résultats des IPs
            for future in concurrent.futures.as_completed(ip_futures):
                result = future.result()
                if result:
                    results.append(('ip', result))
            
            # Collecter les résultats des domaines
            for future in concurrent.futures.as_completed(domain_futures):
                result = future.result()
                if result:
                    results.append(('domain', result))

        # Mettre à jour les activités suspectes avec les résultats
        for result_type, result in results:
            if result_type == 'ip':
                ip = result['ip']
                for activity in self.suspicious_activities:
                    if activity.get("src_ip") == ip or activity.get("dst_ip") == ip:
                        activity["ti_data"] = activity.get("ti_data", {})
                        activity["ti_data"]["alienvault"] = {
                            "pulse_count": result["pulse_count"],
                            "tags": result["tags"]
                        }
                        
                        # Ajuster la sévérité
                        if result["pulse_count"] > 0:
                            threat_tags = {"malware", "c2", "botnet", "ransomware", "apt",
                                         "exploit", "attack", "threat"}
                            if any(tag.lower() in threat_tags for tag in result["tags"]):
                                activity["severity"] = "high"
            
            elif result_type == 'domain':
                domain = result['domain']
                for activity in self.suspicious_activities:
                    if activity.get("query") == domain:
                        activity["ti_data"] = activity.get("ti_data", {})
                        activity["ti_data"]["alienvault"] = {
                            "pulse_count": result["pulse_count"],
                            "tags": result["tags"]
                        }
                        
                        # Ajuster la sévérité
                        if result["pulse_count"] > 0:
                            threat_tags = {"malware", "c2", "botnet", "ransomware", "apt",
                                         "exploit", "attack", "threat"}
                            if any(tag.lower() in threat_tags for tag in result["tags"]):
                                activity["severity"] = "high"

    def _enrich_with_misp(self, ips: Set[str], domains: Set[str]) -> None:
        """
        Enrichit les IPs et domaines avec des données MISP.
        Cette méthode reste inchangée, car elle utilise déjà une approche par lots.
        """
        api_key = self.api_keys.get("misp")
        if not api_key:
            return
        
        headers = {
            'Authorization': api_key,
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        }

        # Préparer une liste d'observables à rechercher
        observables = []
        for ip in ips:
            observables.append({"type": "ip-dst", "value": ip})

        for domain in domains:
            observables.append({"type": "domain", "value": domain})

        # Effectuer une recherche par lots
        try:
            request_data = {
                "returnFormat": "json",
                "limit": 100,
                "page": 1,
                "value": observables
            }

            response = requests.post(MISP_API_URL, headers=headers, json=request_data)

            if response.status_code == 200:
                data = response.json()

                # Traiter les résultats et mettre à jour les activités suspectes
                for attr in data.get("response", {}).get("Attribute", []):
                    attr_type = attr.get("type")
                    attr_value = attr.get("value")

                    if attr_type == "ip-dst":
                        # Mise à jour des activités suspectes concernant cette IP
                        for activity in self.suspicious_activities:
                            if activity.get("src_ip") == attr_value or activity.get("dst_ip") == attr_value:
                                activity["ti_data"] = activity.get("ti_data", {})
                                if "misp" not in activity["ti_data"]:
                                    activity["ti_data"]["misp"] = {
                                        "events": [],
                                        "tags": []
                                    }

                                event_id = attr.get("event_id")
                                if event_id not in [e["id"] for e in activity["ti_data"]["misp"]["events"]]:
                                    activity["ti_data"]["misp"]["events"].append({
                                        "id": event_id,
                                        "info": attr.get("Event", {}).get("info", "N/A")
                                    })

                                for tag in attr.get("Tag", []):
                                    tag_name = tag.get("name")
                                    if tag_name not in activity["ti_data"]["misp"]["tags"]:
                                        activity["ti_data"]["misp"]["tags"].append(tag_name)

                                # Ajuster la sévérité en fonction des tags
                                tags = activity["ti_data"]["misp"]["tags"]
                                if any("tlp:red" in tag.lower() for tag in tags):
                                    activity["severity"] = "critical"
                                elif any("tlp:amber" in tag.lower() for tag in tags):
                                    activity["severity"] = "high"
                                elif any(tag.lower() in {"malware", "ransomware", "apt"} for tag in tags):
                                    activity["severity"] = "high"

                    elif attr_type == "domain":
                        # Mise à jour des activités suspectes concernant ce domaine
                        for activity in self.suspicious_activities:
                            if activity.get("query") == attr_value:
                                activity["ti_data"] = activity.get("ti_data", {})
                                if "misp" not in activity["ti_data"]:
                                    activity["ti_data"]["misp"] = {
                                        "events": [],
                                        "tags": []
                                    }

                                event_id = attr.get("event_id")
                                if event_id not in [e["id"] for e in activity["ti_data"]["misp"]["events"]]:
                                    activity["ti_data"]["misp"]["events"].append({
                                        "id": event_id,
                                        "info": attr.get("Event", {}).get("info", "N/A")
                                    })

                                for tag in attr.get("Tag", []):
                                    tag_name = tag.get("name")
                                    if tag_name not in activity["ti_data"]["misp"]["tags"]:
                                        activity["ti_data"]["misp"]["tags"].append(tag_name)

                                # Ajuster la sévérité en fonction des tags
                                tags = activity["ti_data"]["misp"]["tags"]
                                if any("tlp:red" in tag.lower() for tag in tags):
                                    activity["severity"] = "critical"
                                elif any("tlp:amber" in tag.lower() for tag in tags):
                                    activity["severity"] = "high"
                                elif any(tag.lower() in {"malware", "ransomware", "apt"} for tag in tags):
                                    activity["severity"] = "high"

        except Exception:
            pass

    def _generate_reports(self) -> None:
        """Génère des rapports d'analyse au format JSON et un rapport spécifique pour les fichiers malveillants."""
        try:
            # Créer un timestamp pour le nom de fichier
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"{timestamp}_{os.path.basename(self.pcap_file).split('.')[0]}"

            # Générer un rapport JSON
            json_report = {
                "metadata": {
                    "filename": self.pcap_file,
                    "analysis_time": datetime.datetime.now().isoformat(),
                    "sample_rate": self.sample_rate,
                    "deep_inspection": self.deep_inspection
                },
                "statistics": self.stats,
                "suspicious_activities": sorted(self.suspicious_activities,
                                               key=lambda x: x.get("severity", "low"),
                                               reverse=True),
                "potential_iocs": list(self.potential_iocs),
                "extracted_files": {
                    "total": len(self.extracted_files),
                    "malicious": len(self.malicious_files),
                    "files": self.extracted_files
                }
            }

            # Écrire directement dans le dossier de sortie principal
            json_file_path = os.path.join(self.output_dir, f"{base_filename}_report.json")
            with open(json_file_path, 'w') as f:
                json.dump(json_report, f, indent=2)

            # Générer un rapport spécifique pour les fichiers malveillants si présents
            if self.malicious_files:
                malicious_files_report = os.path.join(self.output_dir, f"{base_filename}_malicious_files.txt")
                with open(malicious_files_report, 'w') as f:
                    for file_info in self.malicious_files:
                        sha256 = file_info.get("sha256", "")
                        filename = file_info.get("filename", "")
                        f.write(f"{sha256}  {filename}\n")

        except Exception:
            pass

    def _is_valid_ip(self, ip: str) -> bool:
        """
        Vérifie si une chaîne est une adresse IP valide.
        """
        try:
            ipaddress.ip_address(ip)
            return True
        except ValueError:
            return False

    def _prepare_results(self, analysis_time: float) -> Dict[str, Any]:
        """
        Prépare les résultats de l'analyse pour le retour.
        """
        # Créer un timestamp pour le nom de fichier
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{timestamp}_{os.path.basename(self.pcap_file).split('.')[0]}"

        # Regrouper les activités suspectes par sévérité
        severity_groups = defaultdict(list)
        for activity in self.suspicious_activities:
            severity = activity.get("severity", "unknown")
            severity_groups[severity].append(activity)

        # Calculer des statistiques sur les IOCs
        ioc_types = Counter()
        for ioc in self.potential_iocs:
            ioc_type = ioc.split(":")[0] if ":" in ioc else "unknown"
            ioc_types[ioc_type] += 1

        # Préparer le résumé des résultats
        results = {
            "metadata": {
                "pcap_file": self.pcap_file,
                "analysis_time": analysis_time,
                "sample_rate": self.sample_rate,
                "deep_inspection": self.deep_inspection
            },
            "statistics": self.stats,
            "summary": {
                "total_suspicious": len(self.suspicious_activities),
                "severity_distribution": {k: len(v) for k, v in severity_groups.items()},
                "top_types": Counter([a.get("type", "unknown") for a in self.suspicious_activities]).most_common(5),
                "ioc_types": dict(ioc_types),
                "scan_count": len(self.scan_activities),
                "ssl_suspicious_count": len([c for c in self.ssl_certs if c.get("self_signed", False) or c.get("blacklisted", False)]),
                "dns_suspicious_count": sum(1 for a in self.suspicious_activities if a.get("type", "").startswith("DNS_")),
                "http_suspicious_count": sum(1 for a in self.suspicious_activities if a.get("type", "").startswith("HTTP_")),
                "kerberos_suspicious_count": sum(1 for a in self.suspicious_activities if a.get("type", "").startswith("KERBEROS_")),
                "smb_suspicious_count": sum(1 for a in self.suspicious_activities if a.get("type", "").startswith("SMB_"))
            },
            "findings": {
                "critical": severity_groups.get("critical", []),
                "high": severity_groups.get("high", [])[:10],  # Limiter pour éviter des résultats trop volumineux
                "medium": severity_groups.get("medium", [])[:5],
                "low": severity_groups.get("low", [])[:3]
            },
            "iocs": list(self.potential_iocs),
            "report_file": os.path.join(self.output_dir, f"{base_filename}_report.json")
        }

        return results


def parse_arguments():
    """
    Parse les arguments de ligne de commande, avec une description améliorée pour le taux d'échantillonnage.
    """
    parser = argparse.ArgumentParser(
        description="Analyseur PCAP optimisé pour la détection de menaces avec intégration MITRE ATT&CK",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument("pcap_file", help="Chemin vers le fichier PCAP à analyser")
    parser.add_argument("-o", "--output-dir", default="results",
                       help="Répertoire de sortie pour les résultats d'analyse")
    parser.add_argument("-s", "--sample-rate", type=float, default=1.0,
                       help="Taux d'échantillonnage (0.0-1.0) - Réduire pour accélérer l'analyse des fichiers volumineux")
    parser.add_argument("-d", "--deep-inspection", action="store_true",
                       help="Effectuer une inspection approfondie (plus lente mais plus précise)")
    parser.add_argument("-v", "--verbose", action="store_true",
                       help="Mode verbeux")
    parser.add_argument("-t", "--threads", type=int, default=None,
                       help="Nombre maximum de threads (par défaut: CPU count + 4)")
    parser.add_argument("--vtotal-key", help="Clé API VirusTotal")
    parser.add_argument("--alienvault-key", help="Clé API AlienVault OTX")
    parser.add_argument("--misp-key", help="Clé API MISP")

    return parser.parse_args()


def main():
    """Fonction principale optimisée."""
    # Parser les arguments
    args = parse_arguments()

    # Désactiver tous les logs si non verbeux
    if not args.verbose:
        logging.getLogger().setLevel(logging.CRITICAL)

    # Collecter les clés API
    api_keys = {}
    if args.vtotal_key:
        api_keys["virustotal"] = args.vtotal_key
    if args.alienvault_key:
        api_keys["alienvault"] = args.alienvault_key
    if args.misp_key:
        api_keys["misp"] = args.misp_key

    start_time = time.time()
    print(f"[+] Démarrage de l'analyse de {args.pcap_file}")
    
    if args.sample_rate < 1.0:
        print(f"[+] Taux d'échantillonnage configuré à {args.sample_rate*100:.1f}% - L'analyse sera plus rapide mais moins exhaustive")

    # Créer l'analyseur PCAP
    analyzer = PcapAnalyzer(
        pcap_file=args.pcap_file,
        output_dir=args.output_dir,
        api_keys=api_keys,
        sample_rate=args.sample_rate,
        deep_inspection=args.deep_inspection,
        max_workers=args.threads
    )

    # Effectuer l'analyse
    try:
        results = analyzer.analyze()
    except KeyboardInterrupt:
        print("\n[!] Analyse interrompue par l'utilisateur")
        sys.exit(1)

    # Vérifier les erreurs
    if "error" in results:
        print(f"[!] Erreur lors de l'analyse: {results['error']}")
        sys.exit(1)

    total_time = time.time() - start_time
    
    # Afficher un résumé des résultats
    print("\n===== Résumé de l'analyse =====")
    print(f"Fichier PCAP: {results['metadata']['pcap_file']}")
    print(f"Temps d'analyse total: {total_time:.2f} secondes")
    print(f"Paquets totaux: {results['statistics']['total_packets']}")
    print(f"Paquets analysés: {results['statistics']['analyzed_packets']} ({results['statistics']['analyzed_packets'] / max(1, results['statistics']['total_packets']) * 100:.1f}%)")

    print("\n=== Activités suspectes détectées ===")
    for severity, count in results["summary"]["severity_distribution"].items():
        print(f"Sévérité {severity.upper()}: {count} activités")

    print("\n=== Top types d'activités suspectes ===")
    for activity_type, count in results["summary"]["top_types"]:
        print(f"{activity_type}: {count}")

    print("\n=== Statistiques d'indicateurs de compromission (IOCs) ===")
    for ioc_type, count in results["summary"]["ioc_types"].items():
        print(f"{ioc_type}: {count}")

    # Ajouter l'affichage d'un échantillon d'IOCs
    if results["iocs"]:
        print("\nExemples d'IOCs détectés:")
        # Afficher jusqu'à 5 IOCs pour chaque type
        iocs_by_type = defaultdict(list)
        for ioc in results["iocs"]:
            ioc_type = ioc.split(":")[0]
            iocs_by_type[ioc_type].append(ioc)

        for ioc_type, iocs in iocs_by_type.items():
            print(f"\n{ioc_type.upper()}:")
            for ioc in iocs[:5]:  # Limiter à 5 exemples par type
                print(f"- {ioc}")

    print(f"\nLe rapport détaillé est disponible dans: {results['report_file']}")

    if results["summary"]["total_suspicious"] > 0:
        print("\n[!] ATTENTION: Des activités suspectes ont été détectées!")

        if results["summary"]["severity_distribution"].get("critical", 0) > 0:
            print("[!!] ALERTE CRITIQUE: Des activités de sévérité critique requièrent une attention immédiate.")

            for activity in results["findings"]["critical"][:3]:  # Afficher les 3 premières alertes critiques
                print(f"- [{activity.get('type', 'N/A')}] {activity.get('src_ip', '')} -> {activity.get('dst_ip', '')}")

    else:
        print("\n[+] Aucune activité suspecte n'a été détectée.")

    # Afficher l'optimisation de vitesse
    file_size_mb = os.path.getsize(args.pcap_file) / (1024 * 1024)
    processing_rate = file_size_mb / total_time
    print(f"\n[+] Performance: {processing_rate:.2f} Mo/s")
    print("[+] Analyse terminée.")


if __name__ == "__main__":
    # Importer les bibliothèques nécessaires pour le threading
    import threading
    main()